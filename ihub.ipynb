{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file 'page_1.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/v4yaaricyyzx\n",
      "Uploaded file 'page_2.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/77n0xera16uw\n",
      "Uploaded file 'page_3.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/dz5qnfgs6io9\n",
      "Uploaded file 'page_4.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/m03sbgefsf7\n",
      "Uploaded file 'page_5.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/py3x2bf49t9b\n",
      "Uploaded file 'page_6.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/iogr2vpxeitf\n",
      "Uploaded file 'page_7.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/b1k0ve8sxypq\n",
      "Uploaded file 'page_8.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/vixsk1qhawes\n",
      "Uploaded file 'page_9.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/tu5y1kds964e\n",
      "Uploaded file 'page_10.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/p7x4s7zuaerb\n",
      "Uploaded file 'page_11.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/qvj8y7q9pi8b\n",
      "Uploaded file 'page_12.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/5dy6i9ng9yli\n",
      "Uploaded file 'page_13.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/3gowgdskcpti\n",
      "Uploaded file 'page_14.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/8sg2i1q7r31d\n",
      "Uploaded file 'page_15.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/u8j038m7leit\n",
      "Uploaded file 'page_16.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/9u032r0dsohb\n",
      "Uploaded file 'page_17.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/jsa4q77v0ya3\n",
      "Uploaded file 'page_18.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/vwlf1hb0twy\n",
      "Uploaded file 'page_19.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/4cco2oez6s8g\n",
      "Uploaded file 'page_20.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/71hs29epfweb\n",
      "Extracted Text:\n",
      " You've outlined a great framework for organizing information from newspaper images. To effectively extract and categorize the data, I'll need a bit more information about your setup and the types of newspaper images you'll be working with:\n",
      "\n",
      "**1. Image Processing Tools:**\n",
      "\n",
      "* **Optical Character Recognition (OCR):**  Will you be using OCR software to convert the image text into machine-readable text? If so, what software are you using? Knowing this will help me determine how accurately the text can be extracted.\n",
      "* **Image Pre-processing:** Are you pre-processing the images (e.g., cropping, resizing, adjusting brightness/contrast) before applying OCR? Any pre-processing techniques could impact the accuracy of text recognition.\n",
      "\n",
      "**2. Newspaper Types and Content:**\n",
      "\n",
      "* **Newspaper Types:**  Are you dealing with local, national, or international newspapers? This will help me anticipate the kinds of information you might find.\n",
      "* **Content Focus:** Do the newspapers generally focus on certain topics (e.g., sports, business, politics)? Knowing this will help me tailor the extraction and categorization process.\n",
      "\n",
      "**3. Data Format:**\n",
      "\n",
      "* **CSV or XLSX:**  Once the information is extracted, will you be exporting it directly to CSV or XLSX format? Or would you prefer a different format for initial processing?\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Once you provide me with this additional information, I can give you a more tailored solution. For instance, let's imagine you're working with images of a local sports newspaper. I might create a script that:\n",
      "\n",
      "1. **Pre-processes the images:**  Crops the image to focus on the relevant article, enhances contrast, and potentially rotates the image if necessary.\n",
      "2. **Applies OCR:** Uses an OCR tool to extract the text from the processed image.\n",
      "3. **Analyzes the text:**  Searches for keywords and phrases related to the sports category (e.g., \"player names\", \"team\", \"score\", \"venue\", \"match date\"). \n",
      "4. **Categorizes the information:**  Organizes the extracted information into a structured format (e.g., a dictionary or a list of dictionaries) for each sports article.\n",
      "5. **Exports the data:** Saves the extracted information in a CSV or XLSX file with appropriate column headers for each category (e.g., Player Name, Team, Venue, Match Date, Outcome).\n",
      "\n",
      "**Let me know your specific needs, and I can provide you with a step-by-step guide and potentially even code examples for your analysis!** \n",
      "\n",
      "Data saved to C:\\Users\\abi04\\OneDrive\\Desktop\\My projects\\New folder\\test_data_2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Gemini API key\n",
    "genai.configure(api_key=\"AIzaSyBYRhdREJVe-7CIKDIxFuVal8RCsSnMFaw\")\n",
    "\n",
    "# Create the model configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "def upload_to_gemini(path, mime_type=\"image/jpeg\"):\n",
    "    \"\"\"Uploads the file to the Gemini API and returns the file URI.\"\"\"\n",
    "    file = genai.upload_file(path, mime_type=mime_type)\n",
    "    print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
    "    return file.uri\n",
    "\n",
    "def analyze_newspaper_images(image_folder):\n",
    "    \"\"\"\n",
    "    Upload images to the Gemini API and extract categorized text.\n",
    "    \"\"\"\n",
    "    image_paths = [os.path.join(image_folder, f\"page_{i}.jpg\") for i in range(1, 21)]\n",
    "    \n",
    "    uploaded_files = [upload_to_gemini(path) for path in image_paths]\n",
    "\n",
    "    # List out the URIs of the uploaded images to pass to the model\n",
    "    uploaded_file_uris = \"\\n\".join(uploaded_files)\n",
    "\n",
    "    prompt = (\n",
    "        \"Youâ€™re an advanced information analyst with extensive experience in extracting and analyzing text data from images of newspapers. Your expertise lies in categorizing information effectively to ensure comprehensive, organized insights across various fields, making it easier for analysis and reporting.\"\n",
    "\n",
    "\"Your task is to extract and organize information from newspaper images.\" \n",
    "\" Categories to Organize Under:\" \n",
    "\"1. Sports: Player Names, Venue, Team/Clubs, Level of Play, Match Date, Outcome, Player Performance Highlights\\n\"\n",
    "\"2. Politics: Politician Names, Political Party, Event Location, Policy Changes, Election Results\\n\"  \n",
    "\"3. Entertainment: Celebrity Names, Movie/Show Titles, Award Nominations, Event Location, Genre\\n\" \n",
    "\"4. Business: Company Names, Financial Reports, Stock Market Trends, Industry Sector, Economic Indicators\\n\"  \n",
    "\"5. Technology: Tech Company Names, Product Launches, Innovation Highlights, Tech Trends, User Reviews\\n\" \n",
    "\"6. Health: Disease/Condition, Health Tips, Research Findings, Medical Guidelines, Health Policy Changes\\n\"\n",
    "\"7. World News: Country Names, Global Events, Humanitarian Issues, Diplomatic Relations, Economic Impact\\n\" \n",
    "\n",
    "\"Please ensure that the extracted information is well-organized and categorized according to the specified categories and is formatted in a way that can be easily converted to XLSX or CSV format.\"\n",
    "    )\n",
    "\n",
    "    # Initialize chat session\n",
    "    chat_session = model.start_chat(history=[])\n",
    "    \n",
    "    # Send prompt with the uploaded image links\n",
    "    response = chat_session.send_message(prompt)\n",
    "\n",
    "    print(\"Extracted Text:\\n\", response.text)  # Print the extracted content for debugging\n",
    "    return response.text\n",
    "\n",
    "def parse_and_categorize_text(extracted_text):\n",
    "    \"\"\"\n",
    "    Parse the extracted text and categorize it into predefined news categories with subcategories.\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"Sports\": {\n",
    "            \"Player Names\": [],\n",
    "            \"Venue\": [],\n",
    "            \"Team/Clubs\": [],\n",
    "            \"Level of Play\": [],\n",
    "            \"Match Date\": [],\n",
    "            \"Match Outcome\": [],\n",
    "            \"Player Performance Highlights\": [],\n",
    "        },\n",
    "        \"Politics\": {\n",
    "            \"Politician Names\": [],\n",
    "            \"Political Party\": [],\n",
    "            \"Event Location\": [],\n",
    "            \"Policy Changes\": [],\n",
    "            \"Election Results\": [],\n",
    "        },\n",
    "        \"Entertainment\": {\n",
    "            \"Celebrity Names\": [],\n",
    "            \"Movie/Show Titles\": [],\n",
    "            \"Award Nominations\": [],\n",
    "            \"Event Location\": [],\n",
    "            \"Genre\": [],\n",
    "        },\n",
    "        \"Business\": {\n",
    "            \"Company Names\": [],\n",
    "            \"Financial Reports\": [],\n",
    "            \"Stock Market Trends\": [],\n",
    "            \"Industry Sector\": [],\n",
    "            \"Economic Indicators\": [],\n",
    "        },\n",
    "        \"Technology\": {\n",
    "            \"Tech Company Names\": [],\n",
    "            \"Product Launches\": [],\n",
    "            \"Innovation Highlights\": [],\n",
    "            \"Tech Trends\": [],\n",
    "            \"User Reviews\": [],\n",
    "        },\n",
    "        \"Health\": {\n",
    "            \"Disease/Condition\": [],\n",
    "            \"Health Tips\": [],\n",
    "            \"Research Findings\": [],\n",
    "            \"Medical Guidelines\": [],\n",
    "            \"Health Policy Changes\": [],\n",
    "        },\n",
    "        \"World News\": {\n",
    "            \"Country Names\": [],\n",
    "            \"Global Events\": [],\n",
    "            \"Humanitarian Issues\": [],\n",
    "            \"Diplomatic Relations\": [],\n",
    "            \"Economic Impact\": [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Split the extracted text into lines and match categories\n",
    "    lines = extracted_text.split(\"\\n\")\n",
    "    current_category = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detect the main category\n",
    "        for category in categories.keys():\n",
    "            if category.lower() in line.lower():\n",
    "                current_category = category\n",
    "                break\n",
    "\n",
    "        # Assign content to subcategories within the current category\n",
    "        if current_category:\n",
    "            for subcategory in categories[current_category]:\n",
    "                if subcategory.lower() in line.lower() and line not in categories[current_category][subcategory]:\n",
    "                    categories[current_category][subcategory].append(line)\n",
    "\n",
    "    return categories\n",
    "\n",
    "def save_to_file(categorized_data, output_path, file_format='xlsx'):\n",
    "    \"\"\"\n",
    "    Saves the categorized data to an Excel or CSV file.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    \n",
    "    for category, subcategories in categorized_data.items():\n",
    "        category_data = []\n",
    "        for subcategory, content in subcategories.items():\n",
    "            if content:  # Only include non-empty content\n",
    "                category_data.append({\n",
    "                    \"Category\": category,\n",
    "                    \"Subcategory\": subcategory,\n",
    "                    \"Content\": \"; \".join(content)\n",
    "                })\n",
    "        if category_data:  # Only create DataFrame if there is data\n",
    "            df = pd.DataFrame(category_data)\n",
    "            dataframes.append(df)\n",
    "\n",
    "    if not dataframes:\n",
    "        print(\"No data to save. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    if file_format == 'xlsx':\n",
    "        final_df.to_excel(output_path, index=False)\n",
    "    elif file_format == 'csv':\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please use 'xlsx' or 'csv'.\")\n",
    "\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "\n",
    "# Folder containing the newspaper images\n",
    "image_folder = r\"C:\\Users\\abi04\\OneDrive\\Desktop\\My projects\\New folder\"\n",
    "\n",
    "# Step 1: Analyze newspaper images\n",
    "extracted_text = analyze_newspaper_images(image_folder)\n",
    "\n",
    "# Step 2: Parse and categorize the text\n",
    "categorized_data = parse_and_categorize_text(extracted_text)\n",
    "\n",
    "# Step 3: Save categorized data to XLSX or CSV\n",
    "output_file_path = r\"C:\\Users\\abi04\\OneDrive\\Desktop\\My projects\\New folder\\test_data_2.xlsx\"\n",
    "save_to_file(categorized_data, output_file_path, file_format='xlsx')  # Change to 'csv' if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newer part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file 'page_3.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/nlj5b6wt0ysd\n",
      "Uploaded file 'page_4.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/b0at9eosvn3s\n",
      "Uploaded file 'page_5.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/4czwgu5fwjyn\n",
      "Uploaded file 'page_6.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/uff95p9k626j\n",
      "Uploaded file 'page_7.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/2u8d4wkszv8u\n",
      "Uploaded file 'page_8.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/myawabtpgvg7\n",
      "Uploaded file 'page_9.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/i1eiu7dkjxlp\n",
      "Uploaded file 'page_10.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/6zhvywtixdck\n",
      "Uploaded file 'page_11.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/k5xkt56agdps\n",
      "Uploaded file 'page_12.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/su0ig1gm08yu\n",
      "Uploaded file 'page_13.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/irlmcp73cdtj\n",
      "Uploaded file 'page_14.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/zv9tdzxlq6cj\n",
      "Uploaded file 'page_15.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/dx37ouqoms8p\n",
      "Uploaded file 'page_16.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/7wj9suawhnvm\n",
      "Uploaded file 'page_17.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/8lhcipurzxlo\n",
      "Uploaded file 'page_18.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/w36yg5d6w3k9\n",
      "Uploaded file 'page_19.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/q7fe0fi3rbqf\n",
      "Uploaded file 'page_20.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/rszixzt9hr2e\n",
      "Extracted Headlines:\n",
      " I understand your request, but I need a bit more information to help you. \n",
      "\n",
      "Here's what I need from you:\n",
      "\n",
      "1. **The images:**  Please provide me with access to the images \"page_3\" to \"page_20\". You can upload them to a file sharing service like Google Drive or Dropbox and share the link with me.\n",
      "2. **A sample of the headlines:**  Please provide me with a few sample headlines from the images to help me understand the formatting and style of the headlines.\n",
      "\n",
      "Once I have access to the images and understand the headline style, I can write a script to extract the text, analyze it, and categorize the headlines according to your categories. \n",
      "\n",
      "Here's how I would approach the task:\n",
      "\n",
      "1. **Image Preprocessing:** I would use OCR (Optical Character Recognition) techniques to convert the image text into machine-readable text.\n",
      "2. **Headline Extraction:** I would use natural language processing (NLP) techniques to identify and extract the headlines from the text. This might involve looking for keywords, specific formatting, or analyzing the sentence structure.\n",
      "3. **Headline Categorization:** I would apply rules based on keywords, phrases, or even topic modeling to categorize the headlines into your specified categories.\n",
      "4. **Output Generation:**  I would then generate the results in a format suitable for an Excel spreadsheet (XLSX) or comma-separated value file (CSV), making it easy for you to analyze and visualize the data.\n",
      "\n",
      "I'm excited to help you extract and analyze those headlines. Please provide me with the necessary information so I can get started! \n",
      "\n",
      "No data to save. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Gemini API key\n",
    "genai.configure(api_key=\"AIzaSyBYRhdREJVe-7CIKDIxFuVal8RCsSnMFaw\")\n",
    "\n",
    "# Create the model configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "def upload_to_gemini(path, mime_type=\"image/jpeg\"):\n",
    "    \"\"\"Uploads the file to the Gemini API and returns the file URI.\"\"\"\n",
    "    file = genai.upload_file(path, mime_type=mime_type)\n",
    "    print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
    "    return file.uri\n",
    "\n",
    "def analyze_newspaper_images(image_folder):\n",
    "    \"\"\"\n",
    "    Upload images to the Gemini API and extract categorized headlines.\n",
    "    \"\"\"\n",
    "    # Load all images named from page_1.jpg to page_20.jpg in the local directory\n",
    "    image_paths = [os.path.join(image_folder, f\"page_{i}.jpg\") for i in range(3, 21)]\n",
    "    \n",
    "    # Upload each image to Gemini API\n",
    "    uploaded_files = [upload_to_gemini(path) for path in image_paths]\n",
    "\n",
    "    # Combine URIs of the uploaded images to pass to the model in the prompt\n",
    "    uploaded_file_uris = \"\\n\".join(uploaded_files)\n",
    "\n",
    "    # Updated prompt to extract headlines based on the categories\n",
    "    prompt = (\n",
    "        \"You are an advanced information analyst tasked with extracting the text of newspaper images ,analyzing and extracting only the headlines. \"\n",
    "        \"I have named these imagesof newspaper as page_3 to page_20\"\n",
    "        \"Please extract the headlines and organize them under the following categories:\\n\"\n",
    "        \"1. city\\n\"\n",
    "        \"2. Politics\\n\"\n",
    "        \"3. Entertainment\\n\"\n",
    "        \"4. Business\\n\"\n",
    "        \"5. Technology\\n\"\n",
    "        \"6. Health\\n\"\n",
    "        \"7. World News\\n\"\n",
    "        \"8.Sports \\n\"\n",
    "        \"Please extract only the relevant headlines under these categories accurately and generate the results which can easily understandable for formating in xlsx or csv.\"\n",
    "    )\n",
    "\n",
    "    # Initialize chat session\n",
    "    chat_session = model.start_chat(history=[])\n",
    "    \n",
    "    # Send prompt with the uploaded image links\n",
    "    response = chat_session.send_message(prompt)\n",
    "\n",
    "    print(\"Extracted Headlines:\\n\", response.text)  # Print the extracted content for debugging\n",
    "    return response.text\n",
    "\n",
    "def parse_and_categorize_headlines(extracted_text):\n",
    "    \"\"\"\n",
    "    Parse the extracted text and categorize it into predefined news categories with subcategories.\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"Sports\": [],\n",
    "        \"Politics\": [],\n",
    "        \"Entertainment\": [],\n",
    "        \"Business\": [],\n",
    "        \"Technology\": [],\n",
    "        \"Health\": [],\n",
    "        \"World News\": []\n",
    "    }\n",
    "\n",
    "    # Split the extracted text into lines and match categories\n",
    "    lines = extracted_text.split(\"\\n\")\n",
    "    current_category = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detect the main category\n",
    "        for category in categories.keys():\n",
    "            if category.lower() in line.lower():\n",
    "                current_category = category\n",
    "                break\n",
    "\n",
    "        # Assign headlines to the current category\n",
    "        if current_category and line and not any(line.lower() in sub.lower() for sub in categories[current_category]):\n",
    "            categories[current_category].append(line)\n",
    "\n",
    "    return categories\n",
    "\n",
    "def save_to_file(categorized_headlines, output_path, file_format='xlsx'):\n",
    "    \"\"\"\n",
    "    Saves the categorized headlines to an Excel or CSV file.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    \n",
    "    for category, headlines in categorized_headlines.items():\n",
    "        if headlines:  # Only include non-empty headlines\n",
    "            category_data = [{\n",
    "                \"Category\": category,\n",
    "                \"Headline\": headline\n",
    "            } for headline in headlines]\n",
    "            df = pd.DataFrame(category_data)\n",
    "            dataframes.append(df)\n",
    "\n",
    "    if not dataframes:\n",
    "        print(\"No data to save. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    if file_format == 'xlsx':\n",
    "        final_df.to_excel(output_path, index=False)\n",
    "    elif file_format == 'csv':\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please use 'xlsx' or 'csv'.\")\n",
    "\n",
    "    print(f\"Headlines saved to {output_path}\")\n",
    "\n",
    "# Folder containing the newspaper images (named as page_1.jpg to page_20.jpg)\n",
    "image_folder = r\"C:\\Users\\abi04\\OneDrive\\Desktop\\My projects\\New folder\"\n",
    "\n",
    "# Step 1: Analyze newspaper images and extract headlines\n",
    "extracted_text = analyze_newspaper_images(image_folder)\n",
    "\n",
    "# Step 2: Parse and categorize the headlines\n",
    "categorized_headlines = parse_and_categorize_headlines(extracted_text)\n",
    "\n",
    "# Step 3: Save categorized headlines to XLSX or CSV\n",
    "output_file_path = r\"C:\\Users\\abi04\\OneDrive\\Desktop\\My projects\\New folder\\new_data.xlsx\"\n",
    "save_to_file(categorized_headlines, output_file_path, file_format='xlsx')  # Change to 'csv' if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file 'page_3.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/lmb0zonkrj7t\n",
      "Uploaded file 'page_4.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/acklf57txafm\n",
      "Uploaded file 'page_5.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/1ttr1isgj3ej\n",
      "Uploaded file 'page_6.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/agbloi2gvwfi\n",
      "Uploaded file 'page_7.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/l44grgyuambb\n",
      "Uploaded file 'page_8.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/53ikmdu8t4ns\n",
      "Uploaded file 'page_9.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/s29eiwt9ika7\n",
      "Uploaded file 'page_10.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/mzzaoamiqtiy\n",
      "Uploaded file 'page_11.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/vyy5usk8jcy\n",
      "Uploaded file 'page_12.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/1mxfuxybebmd\n",
      "Uploaded file 'page_13.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/f8ysptlh39ce\n",
      "Uploaded file 'page_14.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/sobowg8xtpqp\n",
      "Uploaded file 'page_15.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/otek8anv3ffw\n",
      "Uploaded file 'page_16.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/lvqslwuuorcd\n",
      "Uploaded file 'page_17.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/nl45itghebe9\n",
      "Uploaded file 'page_18.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/mbu1pmyszu5h\n",
      "Uploaded file 'page_19.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/z8kdaxpuo2la\n",
      "Uploaded file 'page_20.jpg' as: https://generativelanguage.googleapis.com/v1beta/files/qmpds9p3jkd\n",
      "Extracted Headlines:\n",
      " I understand your request. However, I cannot directly access or analyze external websites or files, including the URIs you provided. My capabilities are limited to processing and generating text based on the information provided to me.\n",
      "\n",
      "To extract headlines from newspaper images and categorize them, you would need to use a combination of tools and techniques:\n",
      "\n",
      "1. **Image-to-Text Conversion:** Tools like Google Cloud Vision API or Amazon Rekognition can be used to extract text from images.\n",
      "2. **Text Processing:** Natural Language Processing (NLP) techniques can be used to identify and extract headlines from the extracted text.\n",
      "3. **Categorization:** Machine learning models or rule-based systems can be used to classify headlines into different categories.\n",
      "\n",
      "You can find resources and tutorials online for each of these steps. \n",
      "\n",
      "Once you have extracted and categorized the headlines, you can then organize them in the desired format (structure=). You can use tools like Python with Pandas or Google Sheets to manipulate the data and convert it to CSV or XLSX format.\n",
      "\n",
      "If you have the extracted text from the images, I can help you with the categorization and formatting steps. \n",
      "\n",
      "Headlines saved to C:\\Users\\abi04\\OneDrive\\Desktop\\My projects\\New folder\\headliness_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Gemini API key\n",
    "genai.configure(api_key=\"AIzaSyBYRhdREJVe-7CIKDIxFuVal8RCsSnMFaw\")\n",
    "\n",
    "# Create the model configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "def upload_to_gemini(path, mime_type=\"image/jpeg\"):\n",
    "    \"\"\"Uploads the file to the Gemini API and returns the file URI.\"\"\"\n",
    "    try:\n",
    "        file = genai.upload_file(path, mime_type=mime_type)\n",
    "        if file:\n",
    "            print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
    "            return file.uri\n",
    "        else:\n",
    "            print(f\"Failed to upload file '{path}'.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file '{path}': {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_newspaper_images(image_folder):\n",
    "    \"\"\"\n",
    "    Upload images to the Gemini API and extract categorized headlines.\n",
    "    \"\"\"\n",
    "    image_paths = [os.path.join(image_folder, f\"page_{i}.jpg\") for i in range(3, 21)]\n",
    "    \n",
    "    # Upload each image to Gemini API\n",
    "    uploaded_files = [upload_to_gemini(path) for path in image_paths]\n",
    "    \n",
    "    # Filter out any failed uploads\n",
    "    uploaded_files = [uri for uri in uploaded_files if uri]\n",
    "\n",
    "    if not uploaded_files:\n",
    "        print(\"No images were successfully uploaded.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Combine URIs of the uploaded images to pass to the model in the prompt\n",
    "    uploaded_file_uris = \"\\n\".join(uploaded_files)\n",
    "\n",
    "    # Updated prompt to extract headlines based on the categories\n",
    "    prompt = (\n",
    "        \"You are an advanced information analyst with extensive experience in extracting and organizing data from various sources, including images and text. Your expertise lies in accurately interpreting information and categorizing it for easy comprehension.\\n\"\n",
    "        \"Your task is to extract the text of the news headlines from the newspaper images I have uploaded, using the following URIs:\\n\"\n",
    "        f\"{uploaded_file_uris}\\n\\n\"\n",
    "        \"Please organize the extracted headlines under the following categories:\\n\"\n",
    "        \"1. Sports\\n\"\n",
    "        \"2. Politics\\n\"\n",
    "        \"3. Entertainment\\n\" \n",
    "        \"4. Business\\n\"\n",
    "        \"5. Technology\\n\"\n",
    "        \"6. Health\\n\"\n",
    "        \"7. World News\\n\"\n",
    "        \"It's essential to maintain the format as structure= for the easy format of the data as we need to convert to xlsx or csv format.\"\n",
    "    )\n",
    "\n",
    "    # Initialize chat session\n",
    "    chat_session = model.start_chat(history=[])\n",
    "    \n",
    "    try:\n",
    "        # Send prompt with the uploaded image links\n",
    "        response = chat_session.send_message(prompt)\n",
    "        print(\"Extracted Headlines:\\n\", response.text)  # Print the extracted content for debugging\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during chat session: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_and_categorize_headlines(extracted_text):\n",
    "    \"\"\"\n",
    "    Parse the extracted text and categorize it into predefined news categories.\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"City\": [],\n",
    "        \"Politics\": [],\n",
    "        \"Entertainment\": [],\n",
    "        \"Business\": [],\n",
    "        \"Technology\": [],\n",
    "        \"Health\": [],\n",
    "        \"World News\": [],\n",
    "        \"Sports\": []\n",
    "    }\n",
    "\n",
    "    # Split the extracted text into lines and detect categories\n",
    "    lines = extracted_text.split(\"\\n\")\n",
    "    current_category = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check if line matches any category\n",
    "        for category in categories:\n",
    "            if category.lower() in line.lower():\n",
    "                current_category = category\n",
    "                break\n",
    "\n",
    "        # Assign the line to the correct category\n",
    "        if current_category and line:\n",
    "            categories[current_category].append(line)\n",
    "\n",
    "    return categories\n",
    "\n",
    "def save_to_file(categorized_headlines, output_path, file_format='xlsx'):\n",
    "    \"\"\"\n",
    "    Save the categorized headlines to an Excel or CSV file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Prepare the data for saving\n",
    "    for category, headlines in categorized_headlines.items():\n",
    "        for headline in headlines:\n",
    "            data.append({\"Category\": category, \"Headline\": headline})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to the appropriate format\n",
    "    if file_format == 'xlsx':\n",
    "        df.to_excel(output_path, index=False)\n",
    "    elif file_format == 'csv':\n",
    "        df.to_csv(output_path, index=False)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please use 'xlsx' or 'csv'.\")\n",
    "\n",
    "    print(f\"Headlines saved to {output_path}\")\n",
    "\n",
    "# Folder containing the newspaper images (named as page_3.jpg to page_20.jpg)\n",
    "image_folder = r\"C:\\Users\\abi04\\OneDrive\\Desktop\\My projects\\New folder\"\n",
    "\n",
    "# extract the newspaper text \n",
    "extracted_text = analyze_newspaper_images(image_folder)\n",
    "\n",
    "if extracted_text:\n",
    "    categorized_headlines = parse_and_categorize_headlines(extracted_text)\n",
    "\n",
    "    \n",
    "    output_file_path = r\"C:\\Users\\abi04\\OneDrive\\Desktop\\My projects\\New folder\\categorize_news_data.xlsx\"\n",
    "    save_to_file(categorized_headlines, output_file_path, file_format='xlsx')  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
